<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Motion Diversification Networks.">
  <meta name="keywords" content="Motion Diversification Networks">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Motion Diversification Networks</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/globe.svg"> -->
  <link rel="icon" href="./data/mdn_icon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Motion Diversification Networks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hee-jae-kim.github.io/">Hee Jae Kim</a>,</span>
            <span class="author-block">
              <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Boston University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="brmod"></span>CVPR 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce Motion Diversification Networks, a novel framework for learning to generate realistic and diverse 3D human motion. Despite recent advances in deep generative motion modeling, existing models often fail to produce samples that capture the full range of plausible and natural 3D human motion within a given context. The lack of diversity becomes even more apparent in applications where subtle and multi-modal 3D human forecasting is crucial for safety, such as robotics and autonomous driving. Towards more realistic and functional 3D motion models, we highlight limitations in existing generative modeling techniques, particularly in overly simplistic latent code sampling strategies. We then introduce a transformer-based diversification mechanism that learns to effectively guide sampling in the latent space. Our proposed attention-based module queries multiple stochastic samples to flexibly predict a diverse set of latent codes which can be subsequently decoded into motion samples. The proposed framework achieves state-of-the-art diversity and accuracy prediction performance across a range of benchmarks and settings, particularly when used to forecast intricate in-the-wild 3D human motion within complex urban environments. 
          </p>
        </div>
      </div>
    </div>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="column">
          <img src="./data/architecture.png" width="100%" />
        </div>
        <div class="content has-text-justified">
          <p class="my-paragraph">
          <p><b>z-transformer:</b>Â We employ an attention-based diversification module to produce a diverse set of latent vectors that expressively model correlations among multiple samples and modes. </p>
          <p><b>Motion Primitives:</b> To guide sample diversity and reduce modeling complexity in diverse scenarios, we incorporate deterministic motion primitives (centroids of clusters in the 3D pose space). </p>
          <p><b>Scene and Social Context:</b> We use the transformer architecture to easily fuse in additional context, i.e., as keys and values in the z-transformer.</p>
<!--           <p><b>Dense Urban Navigation Benchmark:</b> Prior datasets (e.g., Human3.6M, AMASS, and HumanML3D) are limited to static indoor settings. We introduce DenseCity, a simulation benchmark with dense pedestrians. We also use YouTube, which helps further address the current gap between simulated, generated, and realistic 3D human motion.</p> -->
          </p>
        </div>
<!--         <div class="column">
          <img src="./data/architecture.png" width="90%" />
        </div> -->
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Result</h2>
        <div class="content has-text-justified">
          <p class="my-paragraph">
          <b>Evaluation on Humanm3.6M</b> 
          Our method achieves the best-known performance on the Human3.6M benchmark in terms of sample diversity without sacrificing prediction accuracy.
          </p>
        </div>
        <div class="column">
          <img src="./data/eval_H36M.png" width="60%" />
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Result</h2> -->
        <div class="content has-text-justified">
          <p class="my-paragraph">
          <b>Evaluation on HPS and 3DPW</b> 
          Our model shows state-of-the-art results, in terms of diversity and accuracy, for two additional real-world benchmarks. Moreover, our model benefits from diverse data. Specifically, the addition of human motion trajectories extracted from in-the-wild videos (full details in our supplementary) is shown to further improve realism and diversity.
          </p>
        </div>
        <div class="column">
          <img src="./data/eval_Realworld.png" width="100%" />
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Result</h2> -->
        <div class="content has-text-justified">
          <p class="my-paragraph">
          <b>Evaluation on DenseCity</b> 
          Results are shown in terms of diversity (APD) and accuracy (ADE, FDE) against several baselines for both 3D pose and 2D trajectory error over a \textbf{two seconds future prediction} task.
          </p>
        </div>
        <div class="column">
          <img src="./data/eval_Densecity.png" width="100%" />
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Result</h2> -->
        <div class="content has-text-justified">
          <p class="my-paragraph">
          <b>Qualitative Results</b> 
          Visualization of the nuanced and diverse motion predicted by MDN. Specifically, we visualize how 2D paths predicted by our model are context-aware, effectively reasoning over scene layout (e.g., sidewalk or intersection) as well as subtle social interaction (e.g., at close proximity to surrounding pedestrians along the path).  
          </p>
        </div>
        <div class="column">
          <img src="./data/qualitative_results.jpg" width="100%" />
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Supplementary Video</h2>
          <video controls>
          <source src="./data/MDN.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section id=" BibTeX"">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Acknowledgments</h2>
        <div class="content has-text-centered">
          We thank the Rafik B. Hariri Institute for Computing and Computational Science and Engineering (Focused Research Program award \#2023-07-001) and the National Science Foundation (IIS-2152077) for supporting this research.
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a href="./resources/XVO.pdf" class="large-font bottom_buttons">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a href="https://github.com/h2xlab/XVO" class="large-font bottom_buttons">
        <i class="fab fa-github"></i>
      </a> -->
      <br />
      <p>Page template borrowed from <a href="https://https://catdrive.github.io/"><span class="dnerf">CaT</span></a>
    </div>
  </div>
</footer>

</body>
</html>
